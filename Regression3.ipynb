{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "# import plotly as py\n",
    "# import plotly.plotly as plt\n",
    "# import plotly.graph_objs as go\n",
    "# from plotly import __version__\n",
    "from sklearn import preprocessing\n",
    "#from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "#py.tools.set_credentials_file(username='j.carpenter_12', api_key='t07tLoWdgxKH9sS8TH1T')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = datasets.load_boston()\n",
    "names = boston.feature_names\n",
    "target = boston.target\n",
    "\n",
    "bos = pd.DataFrame(boston.data)\n",
    "bos.columns = names\n",
    "bos.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'go' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0b0bda1a1b49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trace = go.Scatter(x=bos.RM,\n\u001b[0m\u001b[1;32m      2\u001b[0m                    \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                    mode ='markers')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'go' is not defined"
     ]
    }
   ],
   "source": [
    "trace = go.Scatter(x=bos.RM,\n",
    "                   y=target,\n",
    "                   mode ='markers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2149ceb17701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.iplot([trace])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###Linear regression\n",
    "###Create training/Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.067815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.577505</td>\n",
       "      <td>0.641607</td>\n",
       "      <td>0.269203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.287234</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.242302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.547998</td>\n",
       "      <td>0.782698</td>\n",
       "      <td>0.348962</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.204470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.242302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.694386</td>\n",
       "      <td>0.599382</td>\n",
       "      <td>0.348962</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.989737</td>\n",
       "      <td>0.063466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.063050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150206</td>\n",
       "      <td>0.658555</td>\n",
       "      <td>0.441813</td>\n",
       "      <td>0.448545</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.066794</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>0.994276</td>\n",
       "      <td>0.033389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.063050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150206</td>\n",
       "      <td>0.687105</td>\n",
       "      <td>0.528321</td>\n",
       "      <td>0.448545</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.066794</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM    ZN     INDUS  CHAS       NOX        RM       AGE       DIS  \\\n",
       "0  0.000000  0.18  0.067815   0.0  0.314815  0.577505  0.641607  0.269203   \n",
       "1  0.000236  0.00  0.242302   0.0  0.172840  0.547998  0.782698  0.348962   \n",
       "2  0.000236  0.00  0.242302   0.0  0.172840  0.694386  0.599382  0.348962   \n",
       "3  0.000293  0.00  0.063050   0.0  0.150206  0.658555  0.441813  0.448545   \n",
       "4  0.000705  0.00  0.063050   0.0  0.150206  0.687105  0.528321  0.448545   \n",
       "\n",
       "        RAD       TAX   PTRATIO         B     LSTAT  \n",
       "0  0.000000  0.208015  0.287234  1.000000  0.089680  \n",
       "1  0.043478  0.104962  0.553191  1.000000  0.204470  \n",
       "2  0.043478  0.104962  0.553191  0.989737  0.063466  \n",
       "3  0.086957  0.066794  0.648936  0.994276  0.033389  \n",
       "4  0.086957  0.066794  0.648936  1.000000  0.099338  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####Normalize data\n",
    "val = bos.values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "val_scaled = min_max_scaler.fit_transform(val)\n",
    "bos_norm = pd.DataFrame(val_scaled,columns=bos.columns)\n",
    "bos_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "X= bos_norm\n",
    "y= target\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.15,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7320210474402874"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'go' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a9991458b570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m trace1 = go.Scatter(x=df['Predicted'],\n\u001b[0m\u001b[1;32m     21\u001b[0m                    \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Actual'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                    mode ='markers')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'go' is not defined"
     ]
    }
   ],
   "source": [
    "###plot lr prediction intercept\n",
    "\n",
    "intercept = lr.intercept_\n",
    "coef = lr.coef_[0]\n",
    "x = X_test['RM']\n",
    "min_x = min(x)\n",
    "max_x = max(x)\n",
    "\n",
    "fit = [intercept, coef*max_x + intercept]\n",
    "\n",
    "df = pd.DataFrame({'Predicted':lr.predict(X_test),'Actual':y_test})\n",
    "\n",
    "\n",
    "z = np.polyfit(x=df.loc[:,'Predicted'],y=df.loc[:,'Actual'],deg=1)\n",
    "p = np.poly1d(z)\n",
    "df['trendline'] = p(df.loc[:,'Predicted'])\n",
    "\n",
    "\n",
    "\n",
    "trace1 = go.Scatter(x=df['Predicted'],\n",
    "                   y=df['Actual'],\n",
    "                   mode ='markers')\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "                  x=[df['Predicted'].min(),df['Predicted'].max()] ,\n",
    "                  y=df['trendline'].sort_values(), \n",
    "                  mode='lines',\n",
    "                  marker=go.Marker(color='rgb(31, 119, 180)'),\n",
    "                  name='Fit'\n",
    "                  )\n",
    "\n",
    "plt.iplot([trace1,trace2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mean Squared Error (MSE) is a measure of how close a fitted line is to data points. For every data point, you take the distance vertically from the point to the corresponding y value on the curve fit (the error), and square the value. Then you add up all those values for all data points, and, in the case of a linear fit, divide by the number of points minus two.** The squaring is done so negative values do not cancel positive values. The smaller the Mean Squared Error, the closer the fit is to the data. The MSE has the units squared of whatever is plotted on the vertical axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.7333069614\n"
     ]
    }
   ],
   "source": [
    "prediction = lr.predict(X_test)\n",
    "\n",
    "##calculate the mean squared error of the model\n",
    "mseFull = np.mean((y_test - lr.predict(X_test))**2)\n",
    "print(mseFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78970128695033248"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##calculate the score of our model predicted/actual\n",
    "lr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feature Selection  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction.\n",
    "\n",
    "The central premise when using a feature selection technique is that the data contains many features that are either redundant or irrelevant, and can thus be removed without incurring much loss of information. Redundant or irrelevant features are two distinct notions, since one relevant feature may be redundant in the presence of another relevant feature with which it is strongly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectFromModel(estimator=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
      "        prefit=False, threshold=0.25)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "sfm = SelectFromModel(lr,threshold=0.25)\n",
    "sfm.fit(X_test,y_test)\n",
    "print(sfm.fit(X_test,y_test))\n",
    "X_transform = sfm.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr.fit(X_transform,y_test)\n",
    "score = lr.score(X_transform,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.131598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257202</td>\n",
       "      <td>0.547040</td>\n",
       "      <td>0.836251</td>\n",
       "      <td>0.137921</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.996470</td>\n",
       "      <td>0.201711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.218109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.127572</td>\n",
       "      <td>0.612569</td>\n",
       "      <td>0.308960</td>\n",
       "      <td>0.268076</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.127863</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.460905</td>\n",
       "      <td>0.464074</td>\n",
       "      <td>0.987642</td>\n",
       "      <td>0.067155</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.797872</td>\n",
       "      <td>0.982879</td>\n",
       "      <td>0.450883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.379399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057613</td>\n",
       "      <td>0.479785</td>\n",
       "      <td>0.050463</td>\n",
       "      <td>0.378079</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.225191</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.984896</td>\n",
       "      <td>0.104581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>0.057141</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.674897</td>\n",
       "      <td>0.524238</td>\n",
       "      <td>0.915551</td>\n",
       "      <td>0.112632</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.970220</td>\n",
       "      <td>0.428808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.453446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106996</td>\n",
       "      <td>0.520789</td>\n",
       "      <td>0.737384</td>\n",
       "      <td>0.265766</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.402672</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>0.941399</td>\n",
       "      <td>0.282561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0.003506</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.346041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.327160</td>\n",
       "      <td>0.450853</td>\n",
       "      <td>0.826982</td>\n",
       "      <td>0.260892</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.223282</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.984366</td>\n",
       "      <td>0.458057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.003199</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.785557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.491770</td>\n",
       "      <td>0.500671</td>\n",
       "      <td>0.934089</td>\n",
       "      <td>0.043858</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.477099</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.977760</td>\n",
       "      <td>0.618929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.045320</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.302469</td>\n",
       "      <td>0.511209</td>\n",
       "      <td>0.904222</td>\n",
       "      <td>0.179114</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.996041</td>\n",
       "      <td>0.307395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.338343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.411523</td>\n",
       "      <td>0.472504</td>\n",
       "      <td>0.790937</td>\n",
       "      <td>0.124453</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.389313</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.347682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.492302</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.339506</td>\n",
       "      <td>0.457942</td>\n",
       "      <td>0.936148</td>\n",
       "      <td>0.160018</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.169847</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.446744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001840</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.271628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.286008</td>\n",
       "      <td>0.468097</td>\n",
       "      <td>0.854789</td>\n",
       "      <td>0.496731</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.236641</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.974305</td>\n",
       "      <td>0.424117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0.514104</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.633745</td>\n",
       "      <td>0.183560</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.048068</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.221771</td>\n",
       "      <td>0.972682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.453446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106996</td>\n",
       "      <td>0.511784</td>\n",
       "      <td>0.523172</td>\n",
       "      <td>0.353236</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.402672</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>0.973524</td>\n",
       "      <td>0.292770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.253666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.411381</td>\n",
       "      <td>0.735324</td>\n",
       "      <td>0.326592</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.190840</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.985451</td>\n",
       "      <td>0.276214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>0.052159</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.471193</td>\n",
       "      <td>0.655106</td>\n",
       "      <td>0.666323</td>\n",
       "      <td>0.127609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.943971</td>\n",
       "      <td>0.274007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.923387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.403292</td>\n",
       "      <td>0.444146</td>\n",
       "      <td>0.956746</td>\n",
       "      <td>0.079722</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.691489</td>\n",
       "      <td>0.955822</td>\n",
       "      <td>0.437362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0.161036</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.648148</td>\n",
       "      <td>0.252730</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041821</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.939533</td>\n",
       "      <td>0.797185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076132</td>\n",
       "      <td>0.826595</td>\n",
       "      <td>0.299691</td>\n",
       "      <td>0.410916</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.129771</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.993267</td>\n",
       "      <td>0.034216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>0.104786</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.674897</td>\n",
       "      <td>0.502778</td>\n",
       "      <td>0.986612</td>\n",
       "      <td>0.102938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.452539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.238270</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.162551</td>\n",
       "      <td>0.513317</td>\n",
       "      <td>0.138002</td>\n",
       "      <td>0.300030</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.068702</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.134106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.218109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.127572</td>\n",
       "      <td>0.559686</td>\n",
       "      <td>0.300721</td>\n",
       "      <td>0.273777</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.127863</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.012639</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.281525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.412340</td>\n",
       "      <td>0.939238</td>\n",
       "      <td>0.282207</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.229008</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.907383</td>\n",
       "      <td>0.575883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386831</td>\n",
       "      <td>0.580954</td>\n",
       "      <td>0.681771</td>\n",
       "      <td>0.122671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.987619</td>\n",
       "      <td>0.219095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0.078861</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.685185</td>\n",
       "      <td>0.468481</td>\n",
       "      <td>0.951596</td>\n",
       "      <td>0.067746</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.806042</td>\n",
       "      <td>0.385486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.090474</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409465</td>\n",
       "      <td>0.357540</td>\n",
       "      <td>0.952626</td>\n",
       "      <td>0.118233</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.888244</td>\n",
       "      <td>0.452815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.453446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106996</td>\n",
       "      <td>0.443188</td>\n",
       "      <td>0.347065</td>\n",
       "      <td>0.306723</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.402672</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>0.997882</td>\n",
       "      <td>0.203366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>0.106860</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.674897</td>\n",
       "      <td>0.606821</td>\n",
       "      <td>0.939238</td>\n",
       "      <td>0.124262</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.016037</td>\n",
       "      <td>0.468543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.371334</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.213992</td>\n",
       "      <td>0.459667</td>\n",
       "      <td>0.918641</td>\n",
       "      <td>0.249843</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.171756</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.990796</td>\n",
       "      <td>0.428808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.338343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.411523</td>\n",
       "      <td>0.427860</td>\n",
       "      <td>0.697219</td>\n",
       "      <td>0.160327</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.389313</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.341336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.012874</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.281525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.410040</td>\n",
       "      <td>0.948507</td>\n",
       "      <td>0.241668</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.229008</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.903853</td>\n",
       "      <td>0.458609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.379399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057613</td>\n",
       "      <td>0.547231</td>\n",
       "      <td>0.038105</td>\n",
       "      <td>0.378079</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.225191</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.966791</td>\n",
       "      <td>0.137693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>0.048808</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.401235</td>\n",
       "      <td>0.499329</td>\n",
       "      <td>0.835221</td>\n",
       "      <td>0.173122</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.401766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.067815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.577505</td>\n",
       "      <td>0.641607</td>\n",
       "      <td>0.269203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.287234</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.271628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.286008</td>\n",
       "      <td>0.469055</td>\n",
       "      <td>0.823893</td>\n",
       "      <td>0.463503</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.236641</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.318433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.105205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119136</td>\n",
       "      <td>0.652807</td>\n",
       "      <td>0.353244</td>\n",
       "      <td>0.374205</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.055344</td>\n",
       "      <td>0.244681</td>\n",
       "      <td>0.988224</td>\n",
       "      <td>0.078918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.013782</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.281525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.494539</td>\n",
       "      <td>0.914521</td>\n",
       "      <td>0.258918</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.229008</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.296921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.616976</td>\n",
       "      <td>0.704428</td>\n",
       "      <td>0.156999</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.375954</td>\n",
       "      <td>0.882979</td>\n",
       "      <td>0.996672</td>\n",
       "      <td>0.163907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>0.006004</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.128666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.390947</td>\n",
       "      <td>0.748994</td>\n",
       "      <td>0.511843</td>\n",
       "      <td>0.158445</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.146947</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.983358</td>\n",
       "      <td>0.039459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0.041220</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.527112</td>\n",
       "      <td>0.504634</td>\n",
       "      <td>0.260264</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.979121</td>\n",
       "      <td>0.244205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>0.063618</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730453</td>\n",
       "      <td>0.509293</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.996949</td>\n",
       "      <td>0.410044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.164589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>0.632305</td>\n",
       "      <td>0.257467</td>\n",
       "      <td>0.362566</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.110687</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.147727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131687</td>\n",
       "      <td>0.541866</td>\n",
       "      <td>0.464470</td>\n",
       "      <td>0.331894</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.114504</td>\n",
       "      <td>0.627660</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.217715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.197947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094650</td>\n",
       "      <td>0.391646</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.620657</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.272901</td>\n",
       "      <td>0.691489</td>\n",
       "      <td>0.980407</td>\n",
       "      <td>0.461645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.171188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139918</td>\n",
       "      <td>0.613336</td>\n",
       "      <td>0.417096</td>\n",
       "      <td>0.623021</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.185115</td>\n",
       "      <td>0.755319</td>\n",
       "      <td>0.996672</td>\n",
       "      <td>0.214404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.027859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.706649</td>\n",
       "      <td>0.195675</td>\n",
       "      <td>0.688103</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.074427</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>0.997554</td>\n",
       "      <td>0.084989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.004894</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.210411</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.244856</td>\n",
       "      <td>0.573098</td>\n",
       "      <td>0.190525</td>\n",
       "      <td>0.204194</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.229008</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.958243</td>\n",
       "      <td>0.056015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.008951</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.281525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.363096</td>\n",
       "      <td>0.347065</td>\n",
       "      <td>0.242514</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.229008</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.727899</td>\n",
       "      <td>0.274834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.045088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>0.578272</td>\n",
       "      <td>0.339856</td>\n",
       "      <td>0.871218</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.427481</td>\n",
       "      <td>0.606383</td>\n",
       "      <td>0.934137</td>\n",
       "      <td>0.103753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.004826</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.371334</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.213992</td>\n",
       "      <td>0.341636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.249652</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.171756</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.589404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.097903</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.633745</td>\n",
       "      <td>0.557578</td>\n",
       "      <td>0.987642</td>\n",
       "      <td>0.054206</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.987594</td>\n",
       "      <td>0.424669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084362</td>\n",
       "      <td>0.525196</td>\n",
       "      <td>0.301751</td>\n",
       "      <td>0.388391</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.179389</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.137693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.091276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088477</td>\n",
       "      <td>0.581337</td>\n",
       "      <td>0.194645</td>\n",
       "      <td>0.388428</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.124046</td>\n",
       "      <td>0.606383</td>\n",
       "      <td>0.996798</td>\n",
       "      <td>0.071468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>0.052405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.674897</td>\n",
       "      <td>0.462732</td>\n",
       "      <td>0.875386</td>\n",
       "      <td>0.131946</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.025619</td>\n",
       "      <td>0.476821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.002046</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.236437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.426327</td>\n",
       "      <td>0.313079</td>\n",
       "      <td>0.361084</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.087786</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.342715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0.064088</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.302469</td>\n",
       "      <td>0.611037</td>\n",
       "      <td>0.741504</td>\n",
       "      <td>0.200247</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.990342</td>\n",
       "      <td>0.165839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>0.143824</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730453</td>\n",
       "      <td>0.439356</td>\n",
       "      <td>0.964985</td>\n",
       "      <td>0.069656</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.605679</td>\n",
       "      <td>0.608720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.053152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057613</td>\n",
       "      <td>0.455068</td>\n",
       "      <td>0.170958</td>\n",
       "      <td>0.859888</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.280534</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947400</td>\n",
       "      <td>0.105960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.453446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106996</td>\n",
       "      <td>0.494156</td>\n",
       "      <td>0.441813</td>\n",
       "      <td>0.269249</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.402672</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>0.974936</td>\n",
       "      <td>0.235651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.430994</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.633745</td>\n",
       "      <td>0.362522</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032736</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.796358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM     ZN     INDUS  CHAS       NOX        RM       AGE       DIS  \\\n",
       "173  0.000961  0.000  0.131598   0.0  0.257202  0.547040  0.836251  0.137921   \n",
       "274  0.000563  0.400  0.218109   1.0  0.127572  0.612569  0.308960  0.268076   \n",
       "491  0.001117  0.000  1.000000   0.0  0.460905  0.464074  0.987642  0.067155   \n",
       "72   0.000959  0.000  0.379399   0.0  0.057613  0.479785  0.050463  0.378079   \n",
       "452  0.057141  0.000  0.646628   0.0  0.674897  0.524238  0.915551  0.112632   \n",
       "76   0.001070  0.000  0.453446   0.0  0.106996  0.520789  0.737384  0.265766   \n",
       "316  0.003506  0.000  0.346041   0.0  0.327160  0.450853  0.826982  0.260892   \n",
       "140  0.003199  0.000  0.785557   0.0  0.491770  0.500671  0.934089  0.043858   \n",
       "471  0.045320  0.000  0.646628   0.0  0.302469  0.511209  0.904222  0.179114   \n",
       "500  0.002451  0.000  0.338343   0.0  0.411523  0.472504  0.790937  0.124453   \n",
       "218  0.001173  0.000  0.492302   1.0  0.339506  0.457942  0.936148  0.160018   \n",
       "9    0.001840  0.125  0.271628   0.0  0.286008  0.468097  0.854789  0.496731   \n",
       "414  0.514104  0.000  0.646628   0.0  0.633745  0.183560  1.000000  0.048068   \n",
       "78   0.000564  0.000  0.453446   0.0  0.106996  0.511784  0.523172  0.353236   \n",
       "323  0.003120  0.000  0.253666   0.0  0.222222  0.411381  0.735324  0.326592   \n",
       "473  0.052159  0.000  0.646628   0.0  0.471193  0.655106  0.666323  0.127609   \n",
       "124  0.001036  0.000  0.923387   0.0  0.403292  0.444146  0.956746  0.079722   \n",
       "388  0.161036  0.000  0.646628   0.0  0.648148  0.252730  1.000000  0.041821   \n",
       "195  0.000084  0.800  0.000000   0.0  0.076132  0.826595  0.299691  0.410916   \n",
       "448  0.104786  0.000  0.646628   0.0  0.674897  0.502778  0.986612  0.102938   \n",
       "271  0.001751  0.200  0.238270   0.0  0.162551  0.513317  0.138002  0.300030   \n",
       "278  0.000826  0.400  0.218109   0.0  0.127572  0.559686  0.300721  0.273777   \n",
       "30   0.012639  0.000  0.281525   0.0  0.314815  0.412340  0.939238  0.282207   \n",
       "501  0.000633  0.000  0.420455   0.0  0.386831  0.580954  0.681771  0.122671   \n",
       "421  0.078861  0.000  0.646628   0.0  0.685185  0.468481  0.951596  0.067746   \n",
       "474  0.090474  0.000  0.646628   0.0  0.409465  0.357540  0.952626  0.118233   \n",
       "79   0.000872  0.000  0.453446   0.0  0.106996  0.443188  0.347065  0.306723   \n",
       "454  0.106860  0.000  0.646628   0.0  0.674897  0.606821  0.939238  0.124262   \n",
       "210  0.001890  0.000  0.371334   1.0  0.213992  0.459667  0.918641  0.249843   \n",
       "497  0.002945  0.000  0.338343   0.0  0.411523  0.427860  0.697219  0.160327   \n",
       "..        ...    ...       ...   ...       ...       ...       ...       ...   \n",
       "33   0.012874  0.000  0.281525   0.0  0.314815  0.410040  0.948507  0.241668   \n",
       "70   0.000921  0.000  0.379399   0.0  0.057613  0.547231  0.038105  0.378079   \n",
       "470  0.048808  0.000  0.646628   0.0  0.401235  0.499329  0.835221  0.173122   \n",
       "0    0.000000  0.180  0.067815   0.0  0.314815  0.577505  0.641607  0.269203   \n",
       "11   0.001249  0.125  0.271628   0.0  0.286008  0.469055  0.823893  0.463503   \n",
       "281  0.000345  0.200  0.105205   0.0  0.119136  0.652807  0.353244  0.374205   \n",
       "22   0.013782  0.000  0.281525   0.0  0.314815  0.494539  0.914521  0.258918   \n",
       "101  0.001214  0.000  0.296921   0.0  0.277778  0.616976  0.704428  0.156999   \n",
       "268  0.006004  0.200  0.128666   0.0  0.390947  0.748994  0.511843  0.158445   \n",
       "485  0.041220  0.000  0.646628   0.0  0.407407  0.527112  0.504634  0.260264   \n",
       "442  0.063618  0.000  0.646628   0.0  0.730453  0.509293  1.000000  0.079586   \n",
       "290  0.000323  0.800  0.164589   0.0  0.053498  0.632305  0.257467  0.362566   \n",
       "84   0.000498  0.000  0.147727   0.0  0.131687  0.541866  0.464470  0.331894   \n",
       "245  0.002079  0.220  0.197947   0.0  0.094650  0.391646  0.693100  0.620657   \n",
       "63   0.001351  0.250  0.171188   0.0  0.139918  0.613336  0.417096  0.623021   \n",
       "55   0.000076  0.900  0.027859   0.0  0.037037  0.706649  0.195675  0.688103   \n",
       "229  0.004894  0.000  0.210411   0.0  0.244856  0.573098  0.190525  0.204194   \n",
       "18   0.008951  0.000  0.281525   0.0  0.314815  0.363096  0.347065  0.242514   \n",
       "351  0.000823  0.600  0.045088   0.0  0.053498  0.578272  0.339856  0.871218   \n",
       "209  0.004826  0.000  0.371334   1.0  0.213992  0.341636  1.000000  0.249652   \n",
       "395  0.097903  0.000  0.646628   0.0  0.633745  0.557578  0.987642  0.054206   \n",
       "82   0.000340  0.250  0.161290   0.0  0.084362  0.525196  0.301751  0.388391   \n",
       "39   0.000240  0.750  0.091276   0.0  0.088477  0.581337  0.194645  0.388428   \n",
       "456  0.052405  0.000  0.646628   0.0  0.674897  0.462732  0.875386  0.131946   \n",
       "46   0.002046  0.000  0.236437   0.0  0.129630  0.426327  0.313079  0.361084   \n",
       "481  0.064088  0.000  0.646628   0.0  0.302469  0.611037  0.741504  0.200247   \n",
       "444  0.143824  0.000  0.646628   0.0  0.730453  0.439356  0.964985  0.069656   \n",
       "355  0.001127  0.800  0.053152   0.0  0.057613  0.455068  0.170958  0.859888   \n",
       "77   0.000908  0.000  0.453446   0.0  0.106996  0.494156  0.441813  0.269249   \n",
       "398  0.430994  0.000  0.646628   0.0  0.633745  0.362522  1.000000  0.032736   \n",
       "\n",
       "          RAD       TAX   PTRATIO         B     LSTAT  \n",
       "173  0.173913  0.208015  0.425532  0.996470  0.201711  \n",
       "274  0.130435  0.127863  0.531915  1.000000  0.049669  \n",
       "491  0.130435  1.000000  0.797872  0.982879  0.450883  \n",
       "72   0.130435  0.225191  0.702128  0.984896  0.104581  \n",
       "452  1.000000  0.914122  0.808511  0.970220  0.428808  \n",
       "76   0.173913  0.402672  0.648936  0.941399  0.282561  \n",
       "316  0.130435  0.223282  0.617021  0.984366  0.458057  \n",
       "140  0.130435  0.477099  0.914894  0.977760  0.618929  \n",
       "471  1.000000  0.914122  0.808511  0.996041  0.307395  \n",
       "500  0.217391  0.389313  0.702128  1.000000  0.347682  \n",
       "218  0.173913  0.169847  0.404255  1.000000  0.446744  \n",
       "9    0.173913  0.236641  0.276596  0.974305  0.424117  \n",
       "414  1.000000  0.914122  0.808511  0.221771  0.972682  \n",
       "78   0.173913  0.402672  0.648936  0.973524  0.292770  \n",
       "323  0.173913  0.190840  0.744681  0.985451  0.276214  \n",
       "473  1.000000  0.914122  0.808511  0.943971  0.274007  \n",
       "124  0.043478  0.001908  0.691489  0.955822  0.437362  \n",
       "388  1.000000  0.914122  0.808511  0.939533  0.797185  \n",
       "195  0.130435  0.129771  0.191489  0.993267  0.034216  \n",
       "448  1.000000  0.914122  0.808511  1.000000  0.452539  \n",
       "271  0.086957  0.068702  0.638298  1.000000  0.134106  \n",
       "278  0.130435  0.127863  0.531915  1.000000  0.150662  \n",
       "30   0.130435  0.229008  0.893617  0.907383  0.575883  \n",
       "501  0.000000  0.164122  0.893617  0.987619  0.219095  \n",
       "421  1.000000  0.914122  0.808511  0.806042  0.385486  \n",
       "474  1.000000  0.914122  0.808511  0.888244  0.452815  \n",
       "79   0.173913  0.402672  0.648936  0.997882  0.203366  \n",
       "454  1.000000  0.914122  0.808511  0.016037  0.468543  \n",
       "210  0.130435  0.171756  0.638298  0.990796  0.428808  \n",
       "497  0.217391  0.389313  0.702128  1.000000  0.341336  \n",
       "..        ...       ...       ...       ...       ...  \n",
       "33   0.130435  0.229008  0.893617  0.903853  0.458609  \n",
       "70   0.130435  0.225191  0.702128  0.966791  0.137693  \n",
       "470  1.000000  0.914122  0.808511  1.000000  0.401766  \n",
       "0    0.000000  0.208015  0.287234  1.000000  0.089680  \n",
       "11   0.173913  0.236641  0.276596  1.000000  0.318433  \n",
       "281  0.173913  0.055344  0.244681  0.988224  0.078918  \n",
       "22   0.130435  0.229008  0.893617  1.000000  0.468819  \n",
       "101  0.173913  0.375954  0.882979  0.996672  0.163907  \n",
       "268  0.173913  0.146947  0.042553  0.983358  0.039459  \n",
       "485  1.000000  0.914122  0.808511  0.979121  0.244205  \n",
       "442  1.000000  0.914122  0.808511  0.996949  0.410044  \n",
       "290  0.130435  0.110687  0.702128  1.000000  0.044150  \n",
       "84   0.086957  0.114504  0.627660  1.000000  0.217715  \n",
       "245  0.260870  0.272901  0.691489  0.980407  0.461645  \n",
       "63   0.304348  0.185115  0.755319  0.996672  0.214404  \n",
       "55   0.173913  0.074427  0.563830  0.997554  0.084989  \n",
       "229  0.304348  0.229008  0.510638  0.958243  0.056015  \n",
       "18   0.130435  0.229008  0.893617  0.727899  0.274834  \n",
       "351  0.130435  0.427481  0.606383  0.934137  0.103753  \n",
       "209  0.130435  0.171756  0.638298  1.000000  0.589404  \n",
       "395  1.000000  0.914122  0.808511  0.987594  0.424669  \n",
       "82   0.130435  0.179389  0.680851  1.000000  0.137693  \n",
       "39   0.086957  0.124046  0.606383  0.996798  0.071468  \n",
       "456  1.000000  0.914122  0.808511  0.025619  0.476821  \n",
       "46   0.086957  0.087786  0.563830  1.000000  0.342715  \n",
       "481  1.000000  0.914122  0.808511  0.990342  0.165839  \n",
       "444  1.000000  0.914122  0.808511  0.605679  0.608720  \n",
       "355  0.130435  0.280534  1.000000  0.947400  0.105960  \n",
       "77   0.173913  0.402672  0.648936  0.974936  0.235651  \n",
       "398  1.000000  0.914122  0.808511  1.000000  0.796358  \n",
       "\n",
       "[76 rows x 13 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_features</th>\n",
       "      <th>MSE</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25.636478</td>\n",
       "      <td>0.607428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_features        MSE     score\n",
       "0           1  25.636478  0.607428"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = sfm.transform(X_test).shape[1]\n",
    "features = pd.DataFrame({'n_features':n_features,'score':score,'MSE':mseFull},index=[0])\n",
    "while n_features > 1:\n",
    "    sfm.threshold += 0.1\n",
    "    X_transform = sfm.transform(X_test)\n",
    "    n_features = X_transform.shape[1]\n",
    "    lr.fit(X_transform,y_test)\n",
    "    score = lr.score(X_transform,y_test)\n",
    "    mseFull = np.mean((y_test - lr.predict(X_transform))**2)\n",
    "    temp = pd.DataFrame({'n_features':n_features,'score':score,'MSE':mseFull},index=[0])\n",
    "    features = features.append(temp)\n",
    "    \n",
    "\n",
    "group_features = features.groupby(features['n_features'],as_index=False).last()\n",
    "group_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can infer that our model performs better the more columns that are included in the linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.6</td>\n",
       "      <td>22.871831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.4</td>\n",
       "      <td>22.871831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.6</td>\n",
       "      <td>12.894565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.8</td>\n",
       "      <td>22.871831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.1</td>\n",
       "      <td>12.894565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual  Predicted\n",
       "0    23.6  22.871831\n",
       "1    32.4  22.871831\n",
       "2    13.6  12.894565\n",
       "3    22.8  22.871831\n",
       "4    16.1  12.894565"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As we are predicting a continuous variable, we import the regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor(random_state = 100,\n",
    "                               max_depth=3, min_samples_leaf=5)\n",
    "dtr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predicted = dtr.predict(X_test)\n",
    "\n",
    "dtr_predictions = pd.DataFrame({'Actual':y_test,'Predicted':predicted})\n",
    "\n",
    "dtr_predictions.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778910370136\n"
     ]
    }
   ],
   "source": [
    "#Model score \n",
    "score = dtr.score(X_test,y_test)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that our basic Decision Tree outperforms the linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using cross validation and grid search to improve models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the decision tree above we can see that there are many differnt options available to set when creating the model. Going through and adjusting each of these by hand to improve our score would take time. This is where cross validation comes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8167652691302683\n",
      "Best parameters: {'max_depth': 6, 'min_samples_leaf': 3}\n",
      "predicted score against actual: 0.898348\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.6</td>\n",
       "      <td>20.844961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.4</td>\n",
       "      <td>31.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.6</td>\n",
       "      <td>16.823810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.8</td>\n",
       "      <td>23.542857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.1</td>\n",
       "      <td>16.823810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual  Predicted\n",
       "0    23.6  20.844961\n",
       "1    32.4  31.291667\n",
       "2    13.6  16.823810\n",
       "3    22.8  23.542857\n",
       "4    16.1  16.823810"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params ={'max_depth':[1,2,3,4,5,6,7,8],\n",
    "         'min_samples_leaf':[1,2,3,4,5]\n",
    "         }\n",
    "grid_search = GridSearchCV(dtr,param_grid=params,cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "\n",
    "predicted = grid_search.predict(X_test)\n",
    "\n",
    "grid_predictions = pd.DataFrame({'Actual':y_test,'Predicted':predicted})\n",
    "score = grid_search.score(X_test,y_test)\n",
    "print(\"predicted score against actual: %f\" % score)\n",
    "grid_predictions.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that by using grid_search to help tune the model for us it has had a large impact, improving the model's accuracy by over ten percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will look at whether a Random Forest model prediction will be more accurate than the linear regression model above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted score against actual: 0.824989\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.6</td>\n",
       "      <td>23.539519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.4</td>\n",
       "      <td>25.743848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.6</td>\n",
       "      <td>16.055184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.8</td>\n",
       "      <td>23.874663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.1</td>\n",
       "      <td>15.265623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual  Predicted\n",
       "0    23.6  23.539519\n",
       "1    32.4  25.743848\n",
       "2    13.6  16.055184\n",
       "3    22.8  23.874663\n",
       "4    16.1  15.265623"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators= 15, random_state = 100,\n",
    "                               max_depth=3, min_samples_leaf=5)\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "predicted = rf.predict(X_test)\n",
    "rf_predictions = pd.DataFrame({'Actual':y_test,'Predicted':predicted})\n",
    "rf_score = rf.score(X_test,y_test)\n",
    "print(\"predicted score against actual: %f\" % rf_score)\n",
    "rf_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search on random forest to tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8579498759563667\n",
      "Best parameters: {'max_depth': 23, 'min_samples_leaf': 1, 'n_estimators': 140}\n",
      "predicted score against actual: 0.886815\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.6</td>\n",
       "      <td>22.313571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.4</td>\n",
       "      <td>30.355000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.6</td>\n",
       "      <td>15.726429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.8</td>\n",
       "      <td>23.316429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.1</td>\n",
       "      <td>16.862857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual  Predicted\n",
       "0    23.6  22.313571\n",
       "1    32.4  30.355000\n",
       "2    13.6  15.726429\n",
       "3    22.8  23.316429\n",
       "4    16.1  16.862857"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params ={'n_estimators':[130,140,150],'max_depth':[20,21,22,23,24,25],\n",
    "         'min_samples_leaf':[1]\n",
    "         }\n",
    "grid_search = GridSearchCV(rf,param_grid=params)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "\n",
    "predicted = grid_search.predict(X_test)\n",
    "\n",
    "grid_predictions = pd.DataFrame({'Actual':y_test,'Predicted':predicted})\n",
    "score = grid_search.score(X_test,y_test)\n",
    "print(\"predicted score against actual: %f\" % score)\n",
    "grid_predictions.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above that although the random forest initially performed better than the decision tree when tuning with grid search the decision tree has created a better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted score against actual: 0.828524\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.6</td>\n",
       "      <td>22.897290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.4</td>\n",
       "      <td>27.030809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.6</td>\n",
       "      <td>17.457687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.8</td>\n",
       "      <td>23.381114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.1</td>\n",
       "      <td>17.428375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual  Predicted\n",
       "0    23.6  22.897290\n",
       "1    32.4  27.030809\n",
       "2    13.6  17.457687\n",
       "3    22.8  23.381114\n",
       "4    16.1  17.428375"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators= 15, random_state = 100,\n",
    "                               max_depth=3, min_samples_leaf=5)\n",
    "\n",
    "\n",
    "gbr.fit(X_train, y_train)\n",
    "predicted = gbr.predict(X_test)\n",
    "gbr_predictions = pd.DataFrame({'Actual':y_test,'Predicted':predicted})\n",
    "gbr_score = gbr.score(X_test,y_test)\n",
    "print(\"predicted score against actual: %f\" % gbr_score)\n",
    "gbr_predictions.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8489300863222985\n",
      "Best parameters: {'learning_rate': 0.1, 'max_depth': 21, 'min_samples_leaf': 3, 'n_estimators': 130}\n",
      "predicted score against actual: 0.916756\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.6</td>\n",
       "      <td>23.295662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.4</td>\n",
       "      <td>31.242559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.6</td>\n",
       "      <td>18.104824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.8</td>\n",
       "      <td>23.785077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.1</td>\n",
       "      <td>17.252985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual  Predicted\n",
       "0    23.6  23.295662\n",
       "1    32.4  31.242559\n",
       "2    13.6  18.104824\n",
       "3    22.8  23.785077\n",
       "4    16.1  17.252985"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params ={'n_estimators':[50,100,130],'max_depth':[20,21,22,23],\n",
    "         'min_samples_leaf':[1,2,3,4,5],'learning_rate':[0.1,0.05,0.02]\n",
    "         }\n",
    "grid_search = GridSearchCV(gbr,param_grid=params)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "\n",
    "predicted = grid_search.predict(X_test)\n",
    "\n",
    "grid_predictions = pd.DataFrame({'Actual':y_test,'Predicted':predicted})\n",
    "score = grid_search.score(X_test,y_test)\n",
    "print(\"predicted score against actual: %f\" % score)\n",
    "grid_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted score against actual: 0.847226\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.6</td>\n",
       "      <td>28.548549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.4</td>\n",
       "      <td>35.951723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.6</td>\n",
       "      <td>14.443148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.8</td>\n",
       "      <td>23.728831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.1</td>\n",
       "      <td>18.489436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual  Predicted\n",
       "0    23.6  28.548549\n",
       "1    32.4  35.951723\n",
       "2    13.6  14.443148\n",
       "3    22.8  23.728831\n",
       "4    16.1  18.489436"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(30,30,30))\n",
    "\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "mlp_predictions = pd.DataFrame({'Actual':y_test,'Predicted':predicted})\n",
    "mlp_score = mlp.score(X_test,y_test)\n",
    "print(\"predicted score against actual: %f\" % mlp_score)\n",
    "mlp_predictions.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.7193964448816771\n",
      "Best parameters: {'learning_rate': 'constant'}\n",
      "predicted score against actual: 0.848918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.6</td>\n",
       "      <td>26.940479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.4</td>\n",
       "      <td>34.782689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.6</td>\n",
       "      <td>16.350471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.8</td>\n",
       "      <td>24.867512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.1</td>\n",
       "      <td>18.945252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual  Predicted\n",
       "0    23.6  26.940479\n",
       "1    32.4  34.782689\n",
       "2    13.6  16.350471\n",
       "3    22.8  24.867512\n",
       "4    16.1  18.945252"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params ={'learning_rate':['constant', 'adaptive']\n",
    "         }\n",
    "grid_search = GridSearchCV(mlp,param_grid=params,cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "\n",
    "predicted = grid_search.predict(X_test)\n",
    "\n",
    "grid_predictions = pd.DataFrame({'Actual':y_test,'Predicted':predicted})\n",
    "score = grid_search.score(X_test,y_test)\n",
    "print(\"predicted score against actual: %f\" % score)\n",
    "grid_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
